{{ define "main" }}

<link rel="stylesheet"
href="https://fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">

<section class="section hero-subheader container-fluid mt-0 py-5 px-0 px-md-3">
  
  <div class="container px-1 px-md-3">
    <div class="row justify-content-center">
      <div class="col-lg-12 text-center">
        <h1 class="mt-0">MedConceptsQA</h1>
      </div>
      <div class="col-lg-12 text-center">
        <p class="lead">
          A Medical Concepts QA Dataset for LLM Evaluation
        </p>
      </div>
    </div>
    
    <div class="row justify-content-center">
      <div class="text-center">
        <a class="btn btn-primary btn-md px-4 mb-2" href="https://www.sciencedirect.com/science/article/pii/S0010482524011740" target="_blank" role="button">
          <svg style="margin-right:3px;" xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-file-text-fill" viewBox="0 0 16 16">
            <path d="M12 0H4a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2zM5 4h6a.5.5 0 0 1 0 1H5a.5.5 0 0 1 0-1zm-.5 2.5A.5.5 0 0 1 5 6h6a.5.5 0 0 1 0 1H5a.5.5 0 0 1-.5-.5zM5 8h6a.5.5 0 0 1 0 1H5a.5.5 0 0 1 0-1zm0 2h3a.5.5 0 0 1 0 1H5a.5.5 0 0 1 0-1z"/>
          </svg>
          Paper
        </a>
        <a class="btn btn-primary btn-md px-4 mb-2" href="https://github.com/nadavlab/MedConceptsQA" target="_blank" role="button">
          <svg style="margin-right:3px;" xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-github" viewBox="0 0 16 16">
            <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
          </svg>
          GitHub
        </a>

        <a class="btn btn-primary btn-md px-4 mb-2" href="https://huggingface.co/datasets/ofir408/MedConceptsQA" target="_blank" role="button">
          <svg style="margin-right:3px;" xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-database" viewBox="0 0 16 16">
            <path d="M4.318 2.687C5.234 2.271 6.536 2 8 2s2.766.27 3.682.687C12.644 3.125 13 3.627 13 4c0 .374-.356.875-1.318 1.313C10.766 5.729 9.464 6 8 6s-2.766-.27-3.682-.687C3.356 4.875 3 4.373 3 4c0-.374.356-.875 1.318-1.313ZM13 5.698V7c0 .374-.356.875-1.318 1.313C10.766 8.729 9.464 9 8 9s-2.766-.27-3.682-.687C3.356 7.875 3 7.373 3 7V5.698c.271.202.58.378.904.525C4.978 6.711 6.427 7 8 7s3.022-.289 4.096-.777A4.92 4.92 0 0 0 13 5.698ZM14 4c0-1.007-.875-1.755-1.904-2.223C11.022 1.289 9.573 1 8 1s-3.022.289-4.096.777C2.875 2.245 2 2.993 2 4v9c0 1.007.875 1.755 1.904 2.223C4.978 15.71 6.427 16 8 16s3.022-.289 4.096-.777C13.125 14.755 14 14.007 14 13V4Zm-1 4.698V10c0 .374-.356.875-1.318 1.313C10.766 11.729 9.464 12 8 12s-2.766-.27-3.682-.687C3.356 10.875 3 10.373 3 10V8.698c.271.202.58.378.904.525C4.978 9.71 6.427 10 8 10s3.022-.289 4.096-.777A4.92 4.92 0 0 0 13 8.698Zm0 3V13c0 .374-.356.875-1.318 1.313C10.766 14.729 9.464 15 8 15s-2.766-.27-3.682-.687C3.356 13.875 3 13.373 3 13v-1.302c.271.202.58.378.904.525C4.978 12.71 6.427 13 8 13s3.022-.289 4.096-.777c.324-.147.633-.323.904-.525Z"/>
          </svg>
          Dataset
        </a>
      </div>
    </div>
    
    <div class="row justify-content-center text-center mt-3 mb-4">
      <div class="col-6 col-md-4">
        <h2 class="mt-3 mb-1">
          484,334
        </h2>
        Procedures QA
      </div>
      <div class="col-6 col-md-4">
        <h2 class="mt-3 mb-1">
          18,818
        </h2>
        Medications QA
      </div>
      <div class="col-6 col-md-4">
        <h2 class="mt-3 mb-1">
          316,680
        </h2>
        Diagnoses QA
      </div>
    </div>

    <div style="text-align: justify; text-justify: inter-word;" class="mt-4"> 
      a dedicated open source benchmark for medical concepts question answering. The benchmark comprises of questions of various medical concepts across different vocabularies: 
      diagnoses, procedures, and drugs. The questions are categorized into three levels of difficulty: 
      easy, medium, and hard. We conducted evaluations of the benchmark using various Large Language Models. 
      Our benchmark serves as a valuable resource for evaluating the abilities of Large Language Models to interpret medical codes and distinguish between medical concepts.
      Our findings showed most of the state-of-the-art CLLMs, despite being pre-trained on medical data, achieved accuracy levels close to random guessing on this benchmark. However, general-purpose models (Llama3-70B and GPT-4) outperformed CLLMs. Notably, GPT-4 exhibited the best performance, although its accuracy remained insufficient for certain datasets in our benchmark.
    
    <!-- Section -->
    <div class="row justify-content-center d-md-block" style="margin-top:100px">
      <h2 class="text-center text-primary">Overview</h2>
      <hr style="width:40%;margin: 0 auto;" class="mb-3"/>
      <div class="col-lg-12 mx-auto">

        <img src="/images/MedConceptsQA_questions_for_levels.jpg" width="2000" height="auto" class="border-0">
      </div>
    </div>
    
    
    
    <div class="row justify-content-center d-md-block" style="margin-top:100px">
      <h2 class="text-center text-primary">MedConceptsQA Benchmark Results</h2>
      <hr style="width:40%;margin: 0 auto;" class="mb-3"/>
      <p>
        We evaluated different models on MedConceptsQA benchmark using our evaluation code that is available <a href="https://github.com/nadavlab/MedConceptsQA" target="_blank">here</a>.
        
        If you wish to submit your model for evaluation, please open a GitHub issue with your model's HuggingFace name <a href="https://github.com/nadavlab/MedConceptsQA/issues" target="_blank">here</a>.
      </p>
      
      <h5>Zero-shot Learning Results:</h5>
      <div class="col-lg-12 mx-auto">
        <table border="1">
          <tr>
              <th>Model Name</th>
              <th>Accuracy</th>
              <th>CI</th>
          </tr>
          <tr>
              <td>gpt-4-0125-preview</td>
              <td>52.489</td>
              <td>3.573</td>
          </tr>
          <tr>
              <td>meta-llama/Meta-Llama-3-70B-Instruct</td>
              <td>47.076</td>
              <td>3.572</td>
          </tr>
          <tr>
              <td>aaditya/Llama3-OpenBioLLM-70B</td>
              <td>41.849</td>
              <td>3.53</td>
          </tr>
          <tr>
              <td>gpt-3.5-turbo</td>
              <td>37.058</td>
              <td>3.456</td>
          </tr>
          <tr>
              <td>meta-llama/Meta-Llama-3-8B-Instruct</td>
              <td>34.8</td>
              <td>3.409</td>
          </tr>
          <tr>
              <td>aaditya/Llama3-OpenBioLLM-8B</td>
              <td>29.431</td>
              <td>3.262</td>
          </tr>
          <tr>
              <td>johnsnowlabs/JSL-MedMNX-7B</td>
              <td>28.649</td>
              <td>3.235</td>
          </tr>
          <tr>
              <td>epfl-llm/meditron-70b</td>
              <td>28.133</td>
              <td>3.218</td>
          </tr>
          <tr>
              <td>dmis-lab/meerkat-7b-v1.0</td>
              <td>27.982</td>
              <td>3.213</td>
          </tr>
          <tr>
              <td>BioMistral/BioMistral-7B-DARE</td>
              <td>26.836</td>
              <td>3.171</td>
          </tr>
          <tr>
              <td>epfl-llm/meditron-7b</td>
              <td>26.107</td>
              <td>3.143</td>
          </tr>
          <tr>
              <td>dmis-lab/biobert-v1.1</td>
              <td>25.636</td>
              <td>3.125</td>
          </tr>
          <tr>
              <td>UFNLP/gatortron-large</td>
              <td>25.298</td>
              <td>3.111</td>
          </tr>
          <tr>
              <td>PharMolix/BioMedGPT-LM-7B</td>
              <td>24.924</td>
              <td>3.095</td>
          </tr>
      </table>
  
      </div>

      <h5 class="mt-3">Few-shot Learning Results:</h5>
      <div class="col-lg-12 mx-auto">
        <table border="1">
          <tr>
              <th>Model Name</th>
              <th>Accuracy</th>
              <th>CI</th>
          </tr>
          <tr>
              <td>gpt-4-0125-preview</td>
              <td>61.911</td>
              <td>3.475</td>
          </tr>
          <tr>
              <td>meta-llama/Meta-Llama-3-70B-Instruct</td>
              <td>57.867</td>
              <td>3.534</td>
          </tr>
          <tr>
              <td>aaditya/Llama3-OpenBioLLM-70B</td>
              <td>53.387</td>
              <td>3.57</td>
          </tr>
          <tr>
              <td>gpt-3.5-turbo</td>
              <td>41.476</td>
              <td>3.526</td>
          </tr>
          <tr>
              <td>meta-llama/Meta-Llama-3-8B-Instruct</td>
              <td>40.693</td>
              <td>3.516</td>
          </tr>
          <tr>
              <td>aaditya/Llama3-OpenBioLLM-8B</td>
              <td>35.316</td>
              <td>3.421</td>
          </tr>
          <tr>
              <td>epfl-llm/meditron-70b</td>
              <td>34.809</td>
              <td>3.409</td>
          </tr>
          <tr>
              <td>johnsnowlabs/JSL-MedMNX-7B</td>
              <td>32.436</td>
              <td>3.35</td>
          </tr>
          <tr>
              <td>BioMistral/BioMistral-7B-DARE</td>
              <td>28.702</td>
              <td>3.237</td>
          </tr>
          <tr>
              <td>PharMolix/BioMedGPT-LM-7B</td>
              <td>28.204</td>
              <td>3.22</td>
          </tr>
          <tr>
              <td>dmis-lab/meerkat-7b-v1.0</td>
              <td>28.187</td>
              <td>3.219</td>
          </tr>
          <tr>
              <td>epfl-llm/meditron-7b</td>
              <td>26.231</td>
              <td>3.148</td>
          </tr>
          <tr>
              <td>dmis-lab/biobert-v1.1</td>
              <td>25.982</td>
              <td>3.138</td>
          </tr>
          <tr>
              <td>UFNLP/gatortron-large</td>
              <td>25.093</td>
              <td>3.102</td>
          </tr>
      </table>
  
      </div>
    </div>
    
  </div>
  
</div>

</div>


</section>
{{ end }}

{{ define "sidebar-prefooter" }}
  
  <div class="container">
    <div class="row justify-content-center">
      <h2>Citation</h2>
      <pre id="arxiv-citation" class="bg-light py-3">
@article{SHOHAM2024109089,
    title = {MedConceptsQA: Open source medical concepts QA benchmark},
    journal = {Computers in Biology and Medicine},
    volume = {182},
    pages = {109089},
    year = {2024},
    issn = {0010-4825},
    doi = {https://doi.org/10.1016/j.compbiomed.2024.109089},
    url = {https://www.sciencedirect.com/science/article/pii/S0010482524011740},
    author = {Ofir Ben Shoham and Nadav Rappoport}
}
      </pre>
    </div>
  </div>
  {{ end }}
